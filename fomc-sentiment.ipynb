{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prequisites\n",
    "\n",
    "[Anaconda](https://www.anaconda.com/) installed\n",
    "\n",
    "[Python 3.9](https://www.python.org/downloads/) installed\n",
    "\n",
    "Note that you will be creating a Python environment with 3.9 in the Setup the Python Environment step\n",
    "\n",
    "A Snowflake account with [Anaconda Packages enabled by ORGADMIN](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-packages.html#using-third-party-packages-from-anaconda). If you do not have a Snowflake account, you can register for a free trial account.\n",
    "\n",
    "A Snowflake account login with a role that has the ability to create database, schema, tables, stages, user-defined functions, and stored procedures. \n",
    "\n",
    "If not, you will need to register for a [free trial](https://signup.snowflake.com/) or use a different role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Environment\n",
    "To create our python environment we run the following in our terminal\n",
    "\n",
    "conda create --name ai-demo --override-channels -c https://repo.anaconda.com/pkgs/snowflake python=3.9 \n",
    "conda activate ai-demo\n",
    "conda install -c https://repo.anaconda.com/pkgs/snowflake snowflake-connector-python snowflake-snowpark-python snowflake snowflake-ml-python xgboost os json cachtools joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Libraries\n",
    "# Snowpark\n",
    "import snowflake.snowpark as snp\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import udf, col, lag, lit, trunc, to_date, replace, last_day, mean, median\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark.types import *\n",
    "from snowflake.snowpark.version import VERSION\n",
    "import snowflake.connector\n",
    "\n",
    "# Snowpark ML\n",
    "# https://docs.snowflake.com/en/developer-guide/snowpark-ml/snowpark-ml-modeling\n",
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.metrics.correlation import correlation\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.registry import model_registry\n",
    "from snowflake.ml._internal.utils import identifier\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "# data sci libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "import cachetools\n",
    "\n",
    "\n",
    "# warning suppresion\n",
    "import warnings; warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Enter Creds, Connect to Snowflake and Create Session\n",
    "\n",
    "# Read credentials\n",
    "with open('creds.json') as f:\n",
    "    connection_parameters = json.load(f)\n",
    "# Connect to a snowflake session\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check or confirm current session settings? \n",
    "session.get_current_database()\n",
    "session.get_current_schema()\n",
    "session.get_current_warehouse()\n",
    "\n",
    "# What version of snowpark are we running?\n",
    "snowpark_version = VERSION\n",
    "\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Press Conference Data \n",
    "\n",
    "The notebook we just ran in the Snowflake cloud saved our scored FOMC press conferences to our database.\n",
    "The next section walks through how to take a csv from github and save it to Snowflake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to Snowflake\n",
    "\n",
    "If you're running this document from github, you will need to load data to your Snowflake account. These two csv's are available on the github repository here: \n",
    "\n",
    "https://github.com/sfc-gh-jregenstein/aifutureoffinance\n",
    "\n",
    "I downloaded them to my local FOMC-ROBERTA folder and here's how to access them and load to Snowflake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data = pd.read_csv(\"FOMC-ROBERTA/pc_data_for_csv.csv\")\n",
    "\n",
    "pc_data.head(5)\n",
    "\n",
    "pc_data =pc_data[['DATE', 'SENTENCE', 'HAWKISH']]\n",
    "\n",
    "session.write_pandas(pc_data, \"PRESS_CONFERENCE_SCORED_UPPER_DATA\", overwrite='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Create Features\n",
    "\n",
    "From here, we're going to do some wrangling on this data that is different from how the paper treats it - which is the power of being able to pull models off of Hugging Face, deploy them locally and then use the resulting data in our pipelines. We get to be creative. \n",
    "\n",
    "Here I'm going to calculate the monthly change in hawkishness for Fed sentiment, so that we can eventually pass that change as a feature to our model for predicting changes in inflation. The wrangling below happens in the cloud, not on my desktop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # PC Scored Data\n",
    "\n",
    "date_win = snp.Window.order_by('DATE')\n",
    "\n",
    "fed_percent_sent =   (\n",
    "    session.table(\"PRESS_CONFERENCE_SCORED_UPPER_DATA\")  \n",
    "    .groupBy(['DATE'])\n",
    "    .agg(\n",
    "    median('HAWKISH').alias('MEDIAN_SENTIMENT'),\n",
    "    mean('HAWKISH').alias('MEAN_SENTIMENT'),\n",
    "    ) \n",
    "    .sort('DATE') \n",
    "   .with_column('LAG_FOMC_MEAN_SENTIMENT', lag('MEAN_SENTIMENT', offset=1) \\\n",
    "    .over(date_win)\n",
    "   )\n",
    "   .with_column('FED_CHANGE', (col('MEAN_SENTIMENT') - col('LAG_FOMC_MEAN_SENTIMENT'))/col('LAG_FOMC_MEAN_SENTIMENT'))  \n",
    "   .with_column('FRED_DATE', last_day('DATE'))\n",
    "   .select('FRED_DATE', 'FED_CHANGE')\n",
    ")\n",
    "\n",
    "fed_percent_sent.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflation Data\n",
    "\n",
    "We're going to explore whether that change in Fed sentiment helps us model changes in inflation. We have only about 60 observations here but our goal is to explore this process and how to create new features and input them into a model. First let's load up our inflation data. This is CPI data, readily available on FRED or in the Snowflake marketplace via Cybersyn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpi_data = session.table(\"CPI_DATA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we join our Fed sentiment data with our Inflation data, aligning the change in Fed sentiment with the change in inflation. In practice we would want to explore many different lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_win = snp.Window.order_by('DATE')\n",
    "\n",
    "fed_percent_sent.show(5)\n",
    "\n",
    "cpi_fed = (\n",
    "    cpi_data \n",
    "    .with_column('LAG_CPI_12', lag('VALUE', offset=12) \\\n",
    "    .over(date_win)\n",
    "   )\n",
    "   .with_column('CPI_CHANGE', (col('VALUE') - col('LAG_CPI_12'))/col('LAG_CPI_12'))  \n",
    "   .join(fed_percent_sent, cpi_data.col('DATE') == fed_percent_sent.col('FRED_DATE'))\n",
    "   .select('DATE', 'FED_CHANGE', 'CPI_CHANGE')\n",
    "   .dropna()\n",
    "\n",
    ")\n",
    "\n",
    "cpi_fed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST\n",
    "\n",
    "Import our press conference data \n",
    "\n",
    "\n",
    "Build a simple XGBoost Regression model\n",
    "What's happening here?\n",
    "\n",
    "The model.fit() function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a Snowpark Optimized Warehouse if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.\n",
    "The model.predict() function actualls creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.use_warehouse('snowpark_opt_wh')\n",
    "\n",
    "# Split the data into train and test sets\n",
    "fomc_train_df, fomc_test_df = cpi_fed.random_split(weights=[0.7, 0.3], seed=0)\n",
    "\n",
    "fomc_train_df.show(5)\n",
    "\n",
    "# Define the XGBRegressor\n",
    "regressor = XGBRegressor(\n",
    "    input_cols=['FED_CHANGE'],\n",
    "    label_cols=['CPI_CHANGE'],\n",
    "    output_cols=['PREDICTED_CPI_CHANGE']\n",
    ")\n",
    "\n",
    "# Train\n",
    "regressor.fit(fomc_train_df)\n",
    "\n",
    "# Predict\n",
    "result = regressor.predict(fomc_test_df)\n",
    "\n",
    "result.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"CPI_CHANGE\", \n",
    "                                        y_pred_col_names=\"PREDICTED_CPI_CHANGE\")\n",
    "\n",
    "result.select(\"CPI_CHANGE\", \"PREDICTED_CPI_CHANGE\").show()\n",
    "print(f\"Mean absolute percentage error: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Plot actual vs predicted \n",
    "g = sns.relplot(data=result[\"CPI_CHANGE\", \"PREDICTED_CPI_CHANGE\"].to_pandas().astype(\"float64\"), x=\"CPI_CHANGE\", y=\"PREDICTED_CPI_CHANGE\", kind=\"scatter\")\n",
    "g.ax.axline((0,0), slope=1, color=\"r\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grid Search\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(),\n",
    "    param_grid={\n",
    "        \"n_estimators\":[100, 200, 300, 400, 500],\n",
    "        \"learning_rate\":[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    },\n",
    "    n_jobs = -1,\n",
    "    scoring=\"neg_mean_absolute_percentage_error\",\n",
    "    input_cols=['FED_CHANGE'],\n",
    "    label_cols=['CPI_CHANGE'],\n",
    "    output_cols=['PREDICTED_CPI_CHANGE']\n",
    ")\n",
    "\n",
    "# Train\n",
    "grid_search.fit(fomc_train_df)\n",
    "\n",
    "\n",
    "# Predict\n",
    "result = grid_search.predict(fomc_test_df)\n",
    "\n",
    "# Analyze results\n",
    "mape = mean_absolute_percentage_error(df=result, \n",
    "                                        y_true_col_names=\"CPI_CHANGE\", \n",
    "                                        y_pred_col_names=\"PREDICTED_CPI_CHANGE\")\n",
    "\n",
    "result.select(\"CPI_CHANGE\", \"PREDICTED_CPI_CHANGE\").show()\n",
    "print(f\"Mean absolute percentage error: {mape}\")\n",
    "\n",
    "\n",
    "# Analyze grid search results\n",
    "gs_results = grid_search.to_sklearn().cv_results_\n",
    "n_estimators_val = []\n",
    "learning_rate_val = []\n",
    "\n",
    "for param_dict in gs_results[\"params\"]:\n",
    "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
    "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
    "mape_val = gs_results[\"mean_test_score\"]*-1\n",
    "\n",
    "gs_results_df = pd.DataFrame(data={\n",
    "    \"n_estimators\":n_estimators_val,\n",
    "    \"learning_rate\":learning_rate_val,\n",
    "    \"mape\":mape_val})\n",
    "\n",
    "\n",
    "\n",
    "# Let's save our optimal model first and its metadata\n",
    "optimal_model = grid_search.to_sklearn().best_estimator_\n",
    "optimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators\n",
    "optimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate\n",
    "\n",
    "optimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &\n",
    "                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Register\n",
    "\n",
    "\n",
    "\n",
    "# Get sample input data to pass into the registry logging function\n",
    "X = fomc_train_df.select('CPI_CHANGE', 'FED_CHANGE').limit(10)\n",
    "\n",
    "db = identifier._get_unescaped_name(session.get_current_database())\n",
    "schema = identifier._get_unescaped_name(session.get_current_schema())\n",
    "\n",
    "# Define model name and version\n",
    "model_name = \"cpi_fed_model\"\n",
    "model_version = 1\n",
    "\n",
    "# Create a registry and log the model\n",
    "registry = model_registry.ModelRegistry(session=session, database_name=db, schema_name=schema, create_if_not_exists=True)\n",
    "\n",
    "registry.log_model(\n",
    "    model_name=model_name,\n",
    "    model_version=model_version,\n",
    "    model=optimal_model,\n",
    "    sample_input_data=X,\n",
    "    options={\"embed_local_ml_library\": True, # This option is enabled to pull latest dev code changes.\n",
    "             \"relax\": True} # relax dependencies\n",
    ")\n",
    "\n",
    "# Add evaluation metric\n",
    "registry.set_metric(model_name=model_name, model_version=model_version, metric_name=\"mean_abs_pct_err\", metric_value=optimal_mape)\n",
    "\n",
    "\n",
    "\n",
    "# Let's confirm it was added\n",
    "registry.list_models().to_pandas()\n",
    "\n",
    "\n",
    "# Pick a deployment name and deploy\n",
    "model_deployment_name = model_name + f\"{model_version}\" + \"_UDF\"\n",
    "\n",
    "registry.deploy(model_name=model_name,\n",
    "                model_version=model_version,\n",
    "                deployment_name=model_deployment_name, \n",
    "                target_method=\"predict\", \n",
    "                permanent=True, \n",
    "                options={\"relax_version\": True})\n",
    "\n",
    "\n",
    "# Let's confirm it was added\n",
    "registry.list_deployments(model_name, model_version).to_pandas()\n",
    "\n",
    "\n",
    "### User our Model for Inference\n",
    "\n",
    "\n",
    "# We can always get a reference to our registry using this function call\n",
    "model_ref = model_registry.ModelReference(registry=registry, model_name=model_name, model_version=model_version)\n",
    "\n",
    "# We can then use the deployed model to perform inference\n",
    "result_sdf = model_ref.predict(deployment_name=model_deployment_name, data=fomc_test_df)\n",
    "result_sdf.rename(F.col('\"output_feature_0\"'),\"PREDICTED_VALUE\").show()\n",
    "\n",
    "\n",
    "session.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
