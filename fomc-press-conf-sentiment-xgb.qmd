---
title: "Untitled"
format: html
---

```{python}
# Snowpark
import snowflake.snowpark as snp
from snowflake.snowpark import functions as F
from snowflake.snowpark.functions import udf, col, lag, lit, trunc, to_date, replace, last_day, mean, median
from snowflake.snowpark.session import Session
from snowflake.snowpark.types import *
from snowflake.snowpark.version import VERSION
import snowflake.connector

# Snowpark ML
import snowflake.ml.modeling.preprocessing as snowml
from snowflake.ml.modeling.pipeline import Pipeline
from snowflake.ml.modeling.metrics.correlation import correlation
from snowflake.ml.modeling.xgboost import XGBRegressor
from snowflake.ml.modeling.model_selection import GridSearchCV
from snowflake.ml.registry import model_registry
from snowflake.ml._internal.utils import identifier
from snowflake.ml.modeling.metrics import mean_absolute_percentage_error


# data sci libraries
import pandas as pd
import os
import json
import numpy as np
import joblib
import cachetools


# warning suppresion
import warnings; warnings.simplefilter('ignore')
```

### Goal or Thesis

label each fed press conference as dovish or hawkish; or on a scale? if we can score each sentence, can we turn this into a dovish/hawkish index? e.g. multiply hawkish by -1....or just take the dove-hawk scoring....that gives us a index over time on the monthly, right? if the change is positive, getting more hawkish...so, we look at the lag, change in fed sentiment...or maybe it's the average change over the last three pressers...strongly hawkish should lead to a rise in yields and weakness in market...


### Enter Creds, Connect to Snowflake and Create Session

```{python}

# Read credentials
with open('creds.json') as f:
    connection_parameters = json.load(f)
# Connect to a snowflake session
session = Session.builder.configs(connection_parameters).create()
```

### Confirm DB, Schema and WH

```{python}

# check or confirm current session settings? 
session.get_current_database()
session.get_current_schema()
session.get_current_warehouse()

# What version of snowpark are we running?
snowpark_version = VERSION

print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))
```

### PC Data

```{python}
 # PC Scored Data
pc_scored_data =  (
    session.table("PRESS_CONFERENCE_SCORED_DATA")
)
pc_scored_data.show(5)

```


```{python}

date_win = snp.Window.order_by('DATE')

fed_percent_sent =   (
    session.table("PRESS_CONFERENCE_SCORED_UPPER_DATA")  
    .groupBy(['DATE'])
    .agg(
    median('HAWKISH').alias('MEDIAN_SENTIMENT'),
    mean('HAWKISH').alias('MEAN_SENTIMENT'),
    ) 
    .sort('DATE') 
   .with_column('LAG_FOMC_MEAN_SENTIMENT', lag('MEAN_SENTIMENT', offset=1) \
    .over(date_win)
   )
   .with_column('FED_CHANGE', (col('MEAN_SENTIMENT') - col('LAG_FOMC_MEAN_SENTIMENT'))/col('LAG_FOMC_MEAN_SENTIMENT'))  
   .with_column('FRED_DATE', last_day('DATE'))
   .select('FRED_DATE', 'FED_CHANGE')
)

fed_percent_sent.show(5)
```

When fed change is positive, mom, language is more hawkish; this should be a negative signal to markets. So, if from October to November, they change to more hawkish, in our data that appears in October and the monthly return in October should be negative or, shall we say, below average relative to the previous 12 months mean monthly return.

# Change DB and Schema to Cybersyn

```{python}
# Set the database using session.sql
session.use_database('CYBERSYN_FINANCIAL__ECONOMIC_ESSENTIALS')
# Set the schema using session.sql
session.use_schema('CYBERSYN')
```


```{python}
def macro_df(session, *fred_symbols):

        macro_data =  (
            session.table("FINANCIAL_FRED_TIMESERIES") 
            .select('VARIABLE', 'DATE', 'VALUE')
        )

        # FRED symbols crosswalk data frame
        fred_ids =  (
            session.table("FINANCIAL_FRED_VARIABLE_SERIES_ID_CROSSWALK")
            .select("VARIABLE", "SERIES_ID")
        )

        # Join the data
        fred_data_ids = (
            fred_ids
            .join(macro_data, fred_ids.col("VARIABLE") == macro_data.col("VARIABLE"))
            .select(fred_ids["VARIABLE"].as_("VARIABLE"), "SERIES_ID", "DATE", "VALUE")
        )

        try:
            fred_symbols[0].split()
        except AttributeError:  # List objects have no split() method.
            fred_symbols = fred_symbols[0]
        # filter to just CPI and =
        final_data = (
            fred_data_ids
            .filter(fred_data_ids['SERIES_ID'].in_(fred_symbols))
            .sort(col("DATE"))
        )

        return final_data
```

```{python}
cpi = (
    macro_df(session, ["CPIAUCSL"]) \
    .select('DATE','VALUE') 
)

cpi_df = cpi.to_pandas()
```


```{python}
# Set the database using session.sql
session.use_database('FOMC_ROBERTA')
# Set the schema using session.sql
session.use_schema('FOMC_SCHEMA')
```

```{python}
session.write_pandas(cpi_df, "CPI_DATA", overwrite='True')
```


```{python}

cpi_data = session.table("CPI_DATA")
date_win = snp.Window.order_by('DATE')

fed_percent_sent.show(5)

cpi_fed = (
    cpi_data 
    .with_column('LAG_CPI_12', lag('VALUE', offset=12) \
    .over(date_win)
   )
   .with_column('CPI_CHANGE', (col('VALUE') - col('LAG_CPI_12'))/col('LAG_CPI_12'))  
   .join(fed_percent_sent, cpi_data.col('DATE') == fed_percent_sent.col('FRED_DATE'))
   .select('DATE', 'FED_CHANGE', 'CPI_CHANGE')
   .dropna()

)

cpi_fed.show(5)
```

# XGBOOST

Import our press conference data 


Build a simple XGBoost Regression model
What's happening here?

The model.fit() function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation. Be sure to use a Snowpark Optimized Warehouse if you need more memory. We are just using an XS Standard Virtual Warehouse here, which we created at the beginning of this quickstart.
The model.predict() function actualls creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data.
You can check the query history once you execute the following cell to check.


```{python}
session.use_warehouse('snowpark_opt_wh')
```
```{python}
# Split the data into train and test sets
fomc_train_df, fomc_test_df = cpi_fed.random_split(weights=[0.7, 0.3], seed=0)

# Run the train and test sets through the Pipeline object we defined earlier
# train_df = preprocessing_pipeline.fit(fomc_train_df).transform(fomc_train_df)
# test_df = preprocessing_pipeline.transform(fomc_test_df)
```

```{python}
fomc_train_df.show(5)
```


```{python}
# Define the XGBRegressor
regressor = XGBRegressor(
    input_cols=['FED_CHANGE'],
    label_cols=['CPI_CHANGE'],
    output_cols=['PREDICTED_CPI_CHANGE']
)
```

```{python}
# Train
regressor.fit(fomc_train_df)
```


```{python}
# Predict
result = regressor.predict(fomc_test_df)

result.to_pandas()
```

```{python}
mape = mean_absolute_percentage_error(df=result, 
                                        y_true_col_names="CPI_CHANGE", 
                                        y_pred_col_names="PREDICTED_CPI_CHANGE")

result.select("CPI_CHANGE", "PREDICTED_CPI_CHANGE").show()
print(f"Mean absolute percentage error: {mape}")
```

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
# Plot actual vs predicted 
g = sns.relplot(data=result["CPI_CHANGE", "PREDICTED_CPI_CHANGE"].to_pandas().astype("float64"), x="CPI_CHANGE", y="PREDICTED_CPI_CHANGE", kind="scatter")
g.ax.axline((0,0), slope=1, color="r")

plt.show()
```

### Grid Search

```{python}
grid_search = GridSearchCV(
    estimator=XGBRegressor(),
    param_grid={
        "n_estimators":[100, 200, 300, 400, 500],
        "learning_rate":[0.1, 0.2, 0.3, 0.4, 0.5],
    },
    n_jobs = -1,
    scoring="neg_mean_absolute_percentage_error",
    input_cols=['FED_CHANGE'],
    label_cols=['CPI_CHANGE'],
    output_cols=['PREDICTED_CPI_CHANGE']
)

# Train
grid_search.fit(fomc_train_df)
```

```{python}
# Predict
result = grid_search.predict(fomc_test_df)

# Analyze results
mape = mean_absolute_percentage_error(df=result, 
                                        y_true_col_names="CPI_CHANGE", 
                                        y_pred_col_names="PREDICTED_CPI_CHANGE")

result.select("CPI_CHANGE", "PREDICTED_CPI_CHANGE").show()
print(f"Mean absolute percentage error: {mape}")
```


```{python}
# Analyze grid search results
gs_results = grid_search.to_sklearn().cv_results_
n_estimators_val = []
learning_rate_val = []
for param_dict in gs_results["params"]:
    n_estimators_val.append(param_dict["n_estimators"])
    learning_rate_val.append(param_dict["learning_rate"])
mape_val = gs_results["mean_test_score"]*-1

gs_results_df = pd.DataFrame(data={
    "n_estimators":n_estimators_val,
    "learning_rate":learning_rate_val,
    "mape":mape_val})
```

```{python}
# Let's save our optimal model first and its metadata
optimal_model = grid_search.to_sklearn().best_estimator_
optimal_n_estimators = grid_search.to_sklearn().best_estimator_.n_estimators
optimal_learning_rate = grid_search.to_sklearn().best_estimator_.learning_rate

optimal_mape = gs_results_df.loc[(gs_results_df['n_estimators']==optimal_n_estimators) &
                                 (gs_results_df['learning_rate']==optimal_learning_rate), 'mape']
```



### Model Register


```{python}
# Get sample input data to pass into the registry logging function
X = fomc_train_df.select('CPI_CHANGE', 'FED_CHANGE').limit(10)

db = identifier._get_unescaped_name(session.get_current_database())
schema = identifier._get_unescaped_name(session.get_current_schema())

# Define model name and version
model_name = "cpi_fed_model"
model_version = 1

# Create a registry and log the model
registry = model_registry.ModelRegistry(session=session, database_name=db, schema_name=schema, create_if_not_exists=True)

registry.log_model(
    model_name=model_name,
    model_version=model_version,
    model=optimal_model,
    sample_input_data=X,
    options={"embed_local_ml_library": True, # This option is enabled to pull latest dev code changes.
             "relax": True} # relax dependencies
)

# Add evaluation metric
registry.set_metric(model_name=model_name, model_version=model_version, metric_name="mean_abs_pct_err", metric_value=optimal_mape)

```


```{python}
# Let's confirm it was added
registry.list_models().to_pandas()
```


```{python}
# Pick a deployment name and deploy
model_deployment_name = model_name + f"{model_version}" + "_UDF"

registry.deploy(model_name=model_name,
                model_version=model_version,
                deployment_name=model_deployment_name, 
                target_method="predict", 
                permanent=True, 
                options={"relax_version": True})
```


```{python}
# Let's confirm it was added
registry.list_deployments(model_name, model_version).to_pandas()
```

### User our Model for Inference

```{python}
# We can always get a reference to our registry using this function call
model_ref = model_registry.ModelReference(registry=registry, model_name=model_name, model_version=model_version)

# We can then use the deployed model to perform inference
result_sdf = model_ref.predict(deployment_name=model_deployment_name, data=fomc_test_df)
result_sdf.rename(F.col('"output_feature_0"'),"PREDICTED_VALUE").show()
```


```{python}
session.close()

```